
@article{sunu_dimensionality_2018,
	title = {Dimensionality reduction for acoustic vehicle classification with spectral embedding},
	url = {http://arxiv.org/abs/1705.09869},
	abstract = {We propose a method for recognizing moving vehicles, using data from roadside audio sensors. This problem has applications ranging widely, from traffic analysis to surveillance. We extract a frequency signature from the audio signal using a short-time Fourier transform, and treat each time window as an individual data point to be classified. By applying a spectral embedding, we decrease the dimensionality of the data sufficiently for K-nearest neighbors to provide accurate vehicle identification.},
	urldate = {2020-08-21},
	journal = {arXiv:1705.09869 [physics, stat]},
	author = {Sunu, Justin and Percus, Allon G.},
	month = feb,
	year = {2018},
	note = {arXiv: 1705.09869},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability}
}

@inproceedings{guera_deepfake_2018,
	address = {Auckland, New Zealand},
	title = {Deepfake {Video} {Detection} {Using} {Recurrent} {Neural} {Networks}},
	isbn = {9781538692943},
	url = {https://ieeexplore.ieee.org/document/8639163/},
	doi = {10.1109/AVSS.2018.8639163},
	urldate = {2020-08-21},
	booktitle = {2018 15th {IEEE} {International} {Conference} on {Advanced} {Video} and {Signal} {Based} {Surveillance} ({AVSS})},
	publisher = {IEEE},
	author = {Guera, David and Delp, Edward J.},
	month = nov,
	year = {2018},
	pages = {1--6}
}

@article{mathie_classification_2004,
	title = {Classification of basic daily movements using a triaxial accelerometer},
	volume = {42},
	issn = {0140-0118, 1741-0444},
	url = {http://link.springer.com/10.1007/BF02347551},
	doi = {10.1007/BF02347551},
	language = {en},
	number = {5},
	urldate = {2020-08-21},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Mathie, M. J. and Celler, B. G. and Lovell, N. H. and Coster, A. C. F.},
	month = sep,
	year = {2004},
	pages = {679--687}
}

@article{turaga_statistical_2011,
	title = {Statistical {Computations} on {Grassmann} and {Stiefel} {Manifolds} for {Image} and {Video}-{Based} {Recognition}},
	volume = {33},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/5740915/},
	doi = {10.1109/TPAMI.2011.52},
	number = {11},
	urldate = {2020-08-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Turaga, P. and Veeraraghavan, A. and Srivastava, A. and Chellappa, R.},
	month = nov,
	year = {2011},
	pages = {2273--2286}
}

@article{jayasumana_kernel_2015,
	title = {Kernel {Methods} on {Riemannian} {Manifolds} with {Gaussian} {RBF} {Kernels}},
	volume = {37},
	issn = {0162-8828, 2160-9292},
	url = {http://arxiv.org/abs/1412.0265},
	doi = {10.1109/TPAMI.2015.2414422},
	abstract = {In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.},
	number = {12},
	urldate = {2020-08-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jayasumana, Sadeep and Hartley, Richard and Salzmann, Mathieu and Li, Hongdong and Harandi, Mehrtash},
	month = dec,
	year = {2015},
	note = {arXiv: 1412.0265},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {2464--2477}
}

@article{zhang_event_1995,
	title = {Event related potentials during object recognition tasks},
	volume = {38},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0361923095020235},
	doi = {10.1016/0361-9230(95)02023-5},
	language = {en},
	number = {6},
	urldate = {2020-08-24},
	journal = {Brain Research Bulletin},
	author = {Zhang, Xiao Lei and Begleiter, Henri and Porjesz, Bernice and Wang, Wenyu and Litke, Ann},
	month = jan,
	year = {1995},
	pages = {531--538}
}

@article{andrzejak_indications_2001,
	title = {Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: {Dependence} on recording region and brain state},
	volume = {64},
	issn = {1063-651X, 1095-3787},
	shorttitle = {Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.64.061907},
	doi = {10.1103/PhysRevE.64.061907},
	language = {en},
	number = {6},
	urldate = {2020-08-24},
	journal = {Physical Review E},
	author = {Andrzejak, Ralph G. and Lehnertz, Klaus and Mormann, Florian and Rieke, Christoph and David, Peter and Elger, Christian E.},
	month = nov,
	year = {2001},
	pages = {061907}
}

@inproceedings{sunu_dimensionality_2018-1,
	address = {Zhuhai},
	title = {Dimensionality reduction for acoustic vehicle classification with spectral embedding},
	isbn = {9781538650530},
	url = {https://ieeexplore.ieee.org/document/8361290/},
	doi = {10.1109/ICNSC.2018.8361290},
	urldate = {2020-08-24},
	booktitle = {2018 {IEEE} 15th {International} {Conference} on {Networking}, {Sensing} and {Control} ({ICNSC})},
	publisher = {IEEE},
	author = {Sunu, Justin and Percus, Allon G.},
	month = mar,
	year = {2018},
	pages = {1--5}
}

@incollection{lieu_signal_2011,
	address = {Boston},
	series = {Applied and {Numerical} {Harmonic} {Analysis}},
	title = {Signal {Ensemble} {Classification} {Using} {Low}-{Dimensional} {Embeddings} and {Earth} {Mover}’s {Distance}},
	isbn = {9780817680954},
	url = {https://doi.org/10.1007/978-0-8176-8095-4_11},
	abstract = {Instead of classifying individual signals, we address classification of objects characterized by signal ensembles (i.e., collections of signals). Such necessity arises frequently in real situations: e.g., classification of video clips or object classification using acoustic scattering experiments to name a few. In particular, we propose an algorithm for classifying signal ensembles by bringing together well-known techniques from various disciplines in a novel way. Our algorithm first performs the dimensionality reduction on training ensembles using either the linear embeddings (e.g., Principal Component Analysis (PCA), Multidimensional Scaling (MDS)) or the nonlinear embeddings (e.g., the Laplacian eigenmap (LE), the diffusion map (DM)). After embedding training ensembles into a lower-dimensional space, our algorithm extends a given test ensemble into the trained embedding space, and then measures the “distance” between the test ensemble and each training ensemble in that space, and classify it using the nearest neighbor method. It turns out that the choice of this ensemble distance measure is critical, and our algorithm adopts the so-called Earth Mover’s Distance (EMD), a robust distance measure successfully used in image retrieval and image registration. We will demonstrate the performance of our algorithm using two real examples: classification of underwater objects using multiple sonar waveforms; and classification of video clips of digit-speaking lips. This article also provides a concise review on the several key concepts in statistical learning such as PCA, MDS, LE, DM, and EMD as well as the practical issues including how to tune parameters, which will be useful for the readers interested in numerical experiments.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Wavelets and {Multiscale} {Analysis}: {Theory} and {Applications}},
	publisher = {Birkhäuser},
	author = {Lieu, Linh and Saito, Naoki},
	editor = {Cohen, Jonathan and Zayed, Ahmed I.},
	year = {2011},
	doi = {10.1007/978-0-8176-8095-4_11},
	keywords = {Principal Component Analysis ,  Dimensionality Reduction ,  Video Frame ,  Hausdorff Distance ,  Spectral Cluster },
	pages = {227--256}
}

@article{chui_signal_2016,
	title = {Signal decomposition and analysis via extraction of frequencies},
	volume = {40},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520315000044},
	doi = {10.1016/j.acha.2015.01.003},
	language = {en},
	number = {1},
	urldate = {2020-08-25},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Chui, Charles K. and Mhaskar, H.N.},
	month = jan,
	year = {2016},
	pages = {97--136}
}

@article{mhaskar_direct_2020,
	title = {A direct approach for function approximation on data defined manifolds},
	url = {http://arxiv.org/abs/1908.00156},
	abstract = {In much of the literature on function approximation by deep networks, the function is assumed to be defined on some known domain, such as a cube or a sphere. In practice, the data might not be dense on these domains, and therefore, the approximation theory results are observed to be too conservative. In manifold learning, one assumes instead that the data is sampled from an unknown manifold; i.e., the manifold is defined by the data itself. Function approximation on this unknown manifold is then a two stage procedure: first, one approximates the Laplace-Beltrami operator (and its eigen-decomposition) on this manifold using a graph Laplacian, and next, approximates the target function using the eigen-functions. Alternatively, one estimates first some atlas on the manifold and then uses local approximation techniques based on the local coordinate charts. In this paper, we propose a more direct approach to function approximation on {\textbackslash}emph\{unknown\}, data defined manifolds without computing the eigen-decomposition of some operator or an atlas for the manifold, and without any kind of training in the classical sense. Our constructions are universal; i.e., do not require the knowledge of any prior on the target function other than continuity on the manifold. We estimate the degree of approximation. For smooth functions, the estimates do not suffer from the so-called saturation phenomenon. We demonstrate via a property called good propagation of errors how the results can be lifted for function approximation using deep networks where each channel evaluates a Gaussian network on a possibly unknown manifold.},
	urldate = {2020-08-25},
	journal = {arXiv:1908.00156 [cs, math, stat]},
	author = {Mhaskar, Hrushikesh},
	month = aug,
	year = {2020},
	note = {arXiv: 1908.00156},
	keywords = {Computer Science - Machine Learning, Mathematics - Functional Analysis, Statistics - Machine Learning}
}