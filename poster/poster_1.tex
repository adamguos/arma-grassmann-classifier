%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Jacobs Landscape Poster
% LaTeX Template
% Version 1.0 (29/03/13)
%
% Created by:
% Computational Physics and Biophysics Group, Jacobs University
% https://teamwork.jacobs-university.de:8443/confluence/display/CoPandBiG/LaTeX+Poster
% 
% Further modified by:
% Nathaniel Johnston (nathaniel@njohnston.ca)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[final]{beamer}

\usepackage[scale=1]{beamerposter} % Use the beamerposter package for laying out the poster

\usetheme{confposter} % Use the confposter theme supplied with this template

\setbeamercolor{block title}{fg=ngreen,bg=white} % Colors of the block titles
\setbeamercolor{block body}{fg=black,bg=white} % Colors of the body of blocks
\setbeamercolor{block alerted title}{fg=white,bg=dblue!70} % Colors of the highlighted block titles
\setbeamercolor{block alerted body}{fg=black,bg=dblue!10} % Colors of the body of highlighted blocks
% Many more colors are available for use in beamerthemeconfposter.sty

%-----------------------------------------------------------
% Define the column widths and overall poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how many columns you want and how much separation you want between columns
% In this template, the separation width chosen is 0.024 of the paper width and a 4-column layout
% onecolwid should therefore be (1-(# of columns+1)*sepwid)/# of columns e.g. (1-(4+1)*0.024)/4 = 0.22
% Set twocolwid to be (2*onecolwid)+sepwid = 0.464
% Set threecolwid to be (3*onecolwid)+2*sepwid = 0.708

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\paperwidth}{48in} % A0 width: 46.8in
\setlength{\paperheight}{36in} % A0 height: 33.1in
\setlength{\sepwid}{0.024\paperwidth} % Separation width (white space) between columns
\setlength{\onecolwid}{0.22\paperwidth} % Width of one column
\setlength{\twocolwid}{0.464\paperwidth} % Width of two columns
\setlength{\threecolwid}{0.708\paperwidth} % Width of three columns
\setlength{\topmargin}{-0.5in} % Reduce the top margin size
%-----------------------------------------------------------

\usepackage{graphicx}  % Required for including images

\usepackage{booktabs} % Top and bottom rules for tables

\usepackage{physics}

\usepackage{tabularx}

%----------------------------------------------------------------------------------------
%	TITLE SECTION 
%----------------------------------------------------------------------------------------

\title{Classifying time series data via frequency decomposition and manifold techniques} % Poster title

\author{Adam Guo, supervised by Prof. Hrushikesh Mhaskar} % Author(s)

\institute{Pomona College ('22), Claremont Graduate University} % Institution(s)

%----------------------------------------------------------------------------------------

\begin{document}

\addtobeamertemplate{block end}{}{\vspace*{2ex}} % White space under blocks
\addtobeamertemplate{block alerted end}{}{\vspace*{2ex}} % White space under highlighted (alert) blocks

\setlength{\belowcaptionskip}{2ex} % White space under figures
\setlength\belowdisplayshortskip{2ex} % White space under equations

\begin{frame}[t] % The whole poster is enclosed in one beamer frame

\begin{columns}[t] % The whole poster consists of three major columns, the second of which is split into two columns twice - the [t] option aligns each column's content to the top

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The first column

%----------------------------------------------------------------------------------------
%   Objectives
%----------------------------------------------------------------------------------------

\begin{alertblock}{Objectives}
    Improve time series classification by:

    \begin{itemize}
        \item Extracting instantaneous freqencies through a novel signal separation operator for use
            as features
        \item Representing time series data as points on a Grassmann manifold via dynamic models and
            using kernel methods on manifolds
    \end{itemize}
\end{alertblock}

%----------------------------------------------------------------------------------------
%   Introduction
%----------------------------------------------------------------------------------------

\begin{block}{Introduction}
    The objective of this research project is to investigate novel solutions to the problem of
    classifying time series data. Time series classification is a general problem widespread in
    signal processing and machine learning, from identifying the sources of sound or radar signals,
    to determining human movements from accelerometer data and detecting algorithmically-generated
    ``deepfake'' video and audio.

    Many machine learning problems can be broadly split into a feature extraction part, where
    relevant information is extracted from the data, and a model fitting part, where a numerical
    model is trained using the extracted features. We investigated two methods for feature
    extraction: a novel signal separation operator (SSO) that extracts intrinsic mode functions
    (IMFs) from signals, and a method for representing signals as points on a Grassmann manifold via
    ARMA parametrisation.  These techniques prove to be effective at capturing relevant information
    from signals, performing well in traditional classification algorithms.
\end{block}

%----------------------------------------------------------------------------------------
%   Signal separation
%----------------------------------------------------------------------------------------

\begin{block}{Signal separation}
    The technique of modeling signals as linear combinations of sinusoids is of fundamental
    importance in signal processing and used widely across many fields. In the big data era, the
    analysis of non-stationary sinusoids is becoming increasingly prevalent, formulated by

    \begin{equation}\label{eq:components}
        f(t) = \sum_{j=1}^K A_j(t) \exp(i\phi_j(t)) + A_0(t)
    \end{equation}

    where $A_j(t)$ and $\phi_j(t)$ denote the amplitude and phase functions respectively, and
    $A_0(t)$ denotes the minimally oscillatory trend of $f(t)$. In \cite{chui_signal_2016}, Chui and
    Mhaskar construct a deep network based on a novel signal separation operator that recovers the
    number $K$ of terms in \ref{eq:components}, the instantaneous frequencies (IFs) $\phi'_j(t)$,
    the amplitude functions $A_j(t)$, and the trend $A_0(t)$.

    We use the first (lowest-energy) IFs extracted as features for classification.
\end{block}

%----------------------------------------------------------------------------------------

\end{column} % End of the first column

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % Begin column 2

%----------------------------------------------------------------------------------------
%   Representation on Grassmann manifold
%----------------------------------------------------------------------------------------

\begin{block}{Representation on Grassmann manifold}
    Traditional machine learning techniques largely assume data that lies in Euclidean space.
    Increasingly, techniques are being developed that account for non-linear geometric constraints
    on sampled data. One such technique is to interpret popular dynamic models of time series data
    as constructing points on the Grassmann manifold (the space of $d$-dimensional subspaces in
    $\mathbb{R}^n$) \cite{turaga_statistical_2011}.

    The autoregressive-moving-average (ARMA) is a well-known dynamic model for time series data that
    parametrises a signal $f(t)$ by the equations

    \begin{equation}
        f(t) = Cz(t) + w(t), \quad w(t) \sim \mathcal{N}(0, R)
    \end{equation}
    \begin{equation}
        z(t + 1) = Az(t) + v(t), \quad v(t) \sim \mathcal{N}(0, Q)
    \end{equation}

    where $z \in \mathbb{R}^d$ is the hidden state vector, $d \leq p$ is the hidden state dimension
    \cite{turaga_statistical_2011}. There are widely-used closed form solutions for estimating the
    parameters $A$ and $C$. It can be shown that the expected observation sequence is given by

    \begin{equation}\label{eq:observation}
        \mathbb{E}[\begin{pmatrix} f(0) & f(1) & f(2) & \cdots \end{pmatrix}^T]
            = \begin{bmatrix} C^T & (CA)^T & (CA^2)^T & \cdots \end{bmatrix}^T z(0)
    \end{equation}

    Hence, the expected observations of $f(t)$ lie in the column space of the observability matrix
    $O_\infty = \begin{bmatrix} C^T & (CA)^T & (CA^2)^T & \cdots \end{bmatrix}^T$, which can be
    approximated by truncating at the $m$th block to form the finite observability matrix $O_m \in
    \mathsf{M}_{mp \times d}$.  Thus, the ARMA model yields a representation of a signal as a
    subspace of Euclidean space, and hence a point on the Grassmann manifold. We store each subspace
    by orthonormalising $O_m$. \cite{turaga_statistical_2011} recommends setting $d$ and $m$ to be
    equal, between 5 and 10.
\end{block}

%----------------------------------------------------------------------------------------
%   Kernel methods on Grassmann manifold
%----------------------------------------------------------------------------------------

\begin{block}{Kernel methods on Grassmann manifold}
    Many machine learning algorithms, such as support vector machines (SVMs) and deep neural
    networks, depend on the Euclidean structure of input data, particularly structures such as norms
    and inner products \cite{jayasumana_kernel_2015}. One way to address this incongruency is to use
    kernel methods that are defined on manifolds. In \cite{jayasumana_kernel_2015} Jayasumana et al.
    generalise the Gaussian radial basis function (RBF) kernel $\exp(-\gamma \norm{x - y}^2)$ to
    manifolds by replacing the Euclidean distance with a positive definite distance function,
    specifically the projection distance on the Grassmann manifold

    \begin{equation}
        d_P([Y_1], [Y_2]) = 2^{-1/2} \norm{Y_1Y_1^T - Y_2Y_2^T}_F
    \end{equation}

    where $[Y_i]$ is the subspace spanned by the columns of $Y_i$, and $Y_1$ and $Y_2$ are
    matrices with orthonormal columns. This yields the projection Gaussian kernel on the Grassmann
    manifold:

    \begin{equation}\label{eq:kernel}
        k_P([Y_1, Y_2]) = \exp(-\gamma d_P^2([Y_1], [Y_2]))
    \end{equation}

    where $\gamma$ is a hyperparameter. This enables the use of algorithms like SVM to classify time
    series data in the form of orthonormal matrices representing points on a Grassmann manifold.
\end{block}

%----------------------------------------------------------------------------------------

\end{column} % End of column 2

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\twocolwid} % Begin a column which is two columns wide (column 3)

%----------------------------------------------------------------------------------------
%   Results
%----------------------------------------------------------------------------------------

\begin{block}{Results}
    \begin{table}
        \begin{tabular}{l r r r r}
            \toprule
            \textbf{Algorithms} & Alcohol EEG & Epilepsy EEG & Vehicle audio & Video digits \\
            \midrule
            IF SVM & 67\% & \textbf{95\%} & & \\
            IF CNN & 69\% & 90\% & & \\
            IF CNN $\rightarrow$ SVM & 71\% & 90\% & & \\
            Grassmann SVM & \textbf{99.8\%} & 66\% & 63\% & \textbf{98\%} \\
            IF + Grassmann SVM & 79\% & 85\% \\
            Grassmann SVM w/ Hermite kernel & 55\% & 64\% & \textbf{68\%} & 40\% \\
            IF + Grassmann SVM  w/ Hermite kernel & 71\% & 68\% & & \\
            \bottomrule
        \end{tabular}
        \caption{Classification accuracies (best performer for each dataset in bold)}
        \label{results}
    \end{table}
\end{block}

%----------------------------------------------------------------------------------------

\begin{columns}[t,totalwidth=\twocolwid] % Split up the two columns wide column

\begin{column}{\onecolwid}\vspace{-.6in} % The first column within column 3 (column 3.1)

%----------------------------------------------------------------------------------------
%   Experiments
%----------------------------------------------------------------------------------------

\begin{block}{Experiments}
    We performed two main experiments using several datasets comprising multivariate time series
    data. First, we performed the signal separation to extract the first instantaneous frequency
    from each signal in the dataset, which we used as input directly to CNN and SVM classifiers. We
    also used the trained CNN to extract features from the time series to classify using an SVM.

    Secondly, we computed the representation of each signal as a point on the Grassmann manifold
    according to (\ref{eq:observation}), and trained an SVM using the kernel function described in
    (\ref{eq:kernel}) to perform classification.

    We performed these tests on four sets of data, using 60\% of each dataset for training and 40\%
    of testing. The optimal hyperparameters used for each are listed.
    \begin{itemize}
        \item SUNY EEG Database Data Set, a set of measurements from 64 electrodes placed on
            the scalp of alcoholic and non-alcoholic subjects \cite{zhang_event_1995} \\
            $d = m = 9, \gamma = 0.2$
        \item Epileptologie Bonn EEG data set, a set of single-channel measurements from epileptic
            and non-epileptic patients \cite{andrzejak_indications_2001}
        \item Audio recordings of different vehicles moving through a parking lot at approximately
            15 mph \cite{sunu_dimensionality_2018}
        \item Video recordings of people speaking numeric digits 0-9 \cite{lieu_signal_2011}
    \end{itemize}

    Note that due to memory constraints, the SSO algorithm could not be run on the vehicle audio or
    video digit datasets.
\end{block}

%----------------------------------------------------------------------------------------

\end{column}    % End column 3.1

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid}\vspace{-.6in}  % Start column 3.2

%----------------------------------------------------------------------------------------
%   Conclusions
%----------------------------------------------------------------------------------------

\begin{block}{Conclusions}
    We conclude that the approach of representing time series data as points on a Grassmann manifold
    via ARMA parameterisation, then applying a manifold kernel SVM for classification is highly
    effective for datasets with a high number of features. The alcohol EEG dataset comprises 64
    channels while the video digits dataset has effectively 3,850 channels, one for each pixel of
    the frame.

    The Grassmann SVM approach performs relatively poorly on datasets with fewer features, such as
    the epilepsy EEG dataset and the vehicle audio dataset, which comprise 1 and 2 features
    respectively. In the case of the epilepsy EEG dataset, using instantaneous frequency extraction
    as an input to other classification algorithms such as SVM and CNN performs relatively well. It
    is notable that combining the IF and Grassmann SVM approaches splits the difference in
    performance between the two approaches.
\end{block}

%----------------------------------------------------------------------------------------
%   References
%----------------------------------------------------------------------------------------

\begin{block}{References}

% \nocite{*} % Insert publications even if they are not cited in the poster
\footnotesize{\bibliographystyle{unsrt}
\bibliography{citations}\vspace{0.75in}}

\end{block}

%----------------------------------------------------------------------------------------

\end{column}    % End column 3.2

\end{columns}   % End split within column 3

\end{column}    % End column 3

%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

% \setbeamercolor{block title}{fg=red,bg=white} % Change the block title color
% 
% \begin{block}{Acknowledgements}
% 
% \small{\rmfamily{Nam mollis tristique neque eu luctus. Suspendisse rutrum congue nisi sed convallis. Aenean id neque dolor. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.}} \\
% 
% \end{block}

%----------------------------------------------------------------------------------------
%	CONTACT INFORMATION
%----------------------------------------------------------------------------------------

% \setbeamercolor{block alerted title}{fg=black,bg=norange} % Change the alert block title colors
% \setbeamercolor{block alerted body}{fg=black,bg=white} % Change the alert block body colors
% 
% \begin{alertblock}{Contact Information}
% 
% \begin{itemize}
% \item Web: \href{http://www.university.edu/smithlab}{http://www.university.edu/smithlab}
% \item Email: \href{mailto:john@smith.com}{john@smith.com}
% \item Phone: +1 (000) 111 1111
% \end{itemize}
% 
% \end{alertblock}
% 
% \begin{center}
% \begin{tabular}{ccc}
% \includegraphics[width=0.4\linewidth]{logo.png} & \hfill & \includegraphics[width=0.4\linewidth]{logo.png}
% \end{tabular}
% \end{center}

% \begin{column}{\onecolwid}\vspace{-.6in} % The second column within column 2 (column 2.2)
% 
% \end{column} % End of column 2.2

% \end{columns} % End of the split of column 2 - any content after this will now take up 2 columns width

%----------------------------------------------------------------------------------------
%	IMPORTANT RESULT
%----------------------------------------------------------------------------------------

% \begin{alertblock}{Important Result}
% 
% Lorem ipsum dolor \textbf{sit amet}, consectetur adipiscing elit. Sed commodo molestie porta. Sed ultrices scelerisque sapien ac commodo. Donec ut volutpat elit.
% 
% \end{alertblock} 

%----------------------------------------------------------------------------------------

% \begin{columns}[t,totalwidth=\twocolwid] % Split up the two columns wide column again
% 
% \begin{column}{\onecolwid} % The first column within column 2 (column 2.1)


%----------------------------------------------------------------------------------------

% \end{column} % End of column 2.1
% 
% \begin{column}{\onecolwid} % The second column within column 2 (column 2.2)

%----------------------------------------------------------------------------------------
%	RESULTS
%----------------------------------------------------------------------------------------

% \begin{block}{Results}
% 
% \begin{figure}
% \includegraphics[width=0.8\linewidth]{placeholder.jpg}
% \caption{Figure caption}
% \end{figure}
% 
% Nunc tempus venenatis facilisis. Curabitur suscipit consequat eros non porttitor. Sed a massa dolor, id ornare enim:
% 
% \begin{table}
% \vspace{2ex}
% \begin{tabular}{l l l}
% \toprule
% \textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
% \midrule
% Treatment 1 & 0.0003262 & 0.562 \\
% Treatment 2 & 0.0015681 & 0.910 \\
% Treatment 3 & 0.0009271 & 0.296 \\
% \bottomrule
% \end{tabular}
% \caption{Table caption}
% \end{table}
% 
% \end{block}

%----------------------------------------------------------------------------------------

% \end{column} % End of column 2.2
% 
% \end{columns} % End of the split of column 2
% 
% \end{column} % End of the second column

%----------------------------------------------------------------------------------------

\end{columns} % End of all the columns in the poster

\end{frame} % End of the enclosing frame

\end{document}
