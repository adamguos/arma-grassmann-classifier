%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Jacobs Landscape Poster
% LaTeX Template
% Version 1.0 (29/03/13)
%
% Created by:
% Computational Physics and Biophysics Group, Jacobs University
% https://teamwork.jacobs-university.de:8443/confluence/display/CoPandBiG/LaTeX+Poster
% 
% Further modified by:
% Nathaniel Johnston (nathaniel@njohnston.ca)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[final]{beamer}

\usepackage[scale=1.2]{beamerposter} % Use the beamerposter package for laying out the poster

\usetheme{confposter} % Use the confposter theme supplied with this template

\setbeamercolor{block title}{fg=ngreen,bg=white} % Colors of the block titles
\setbeamercolor{block body}{fg=black,bg=white} % Colors of the body of blocks
\setbeamercolor{block alerted title}{fg=white,bg=dblue!70} % Colors of the highlighted block titles
\setbeamercolor{block alerted body}{fg=black,bg=dblue!10} % Colors of the body of highlighted blocks
% Many more colors are available for use in beamerthemeconfposter.sty

%-----------------------------------------------------------
% Define the column widths and overall poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how many columns you want and how much separation you want between columns
% In this template, the separation width chosen is 0.024 of the paper width and a 4-column layout
% onecolwid should therefore be (1-(# of columns+1)*sepwid)/# of columns e.g. (1-(4+1)*0.024)/4 = 0.22
% Set twocolwid to be (2*onecolwid)+sepwid = 0.464
% Set threecolwid to be (3*onecolwid)+2*sepwid = 0.708

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\paperwidth}{48in} % A0 width: 46.8in
\setlength{\paperheight}{36in} % A0 height: 33.1in
\setlength{\sepwid}{0.024\paperwidth} % Separation width (white space) between columns
\setlength{\onecolwid}{0.22\paperwidth} % Width of one column
\setlength{\twocolwid}{0.464\paperwidth} % Width of two columns
\setlength{\threecolwid}{0.708\paperwidth} % Width of three columns
\setlength{\topmargin}{-0.5in} % Reduce the top margin size
%-----------------------------------------------------------

\usepackage{graphicx}  % Required for including images

\usepackage{booktabs} % Top and bottom rules for tables

\usepackage{physics}

\usepackage{tabularx}

\newcommand{\lbr}{\vspace{0.5in}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION 
%----------------------------------------------------------------------------------------

\title{Classifying time series data via frequency decomposition and manifold techniques} % Poster title

\author{Adam Guo, supervised by Prof. Hrushikesh Mhaskar} % Author(s)

\institute{Pomona College ('22), Claremont Graduate University} % Institution(s)

%----------------------------------------------------------------------------------------

\begin{document}

\addtobeamertemplate{block end}{}{\vspace*{2ex}} % White space under blocks
\addtobeamertemplate{block alerted end}{}{\vspace*{2ex}} % White space under highlighted (alert) blocks

\setlength{\belowcaptionskip}{2ex} % White space under figures
\setlength\belowdisplayshortskip{2ex} % White space under equations

\begin{frame}[t] % The whole poster is enclosed in one beamer frame

\begin{columns}[t] % The whole poster consists of three major columns, the second of which is split into two columns twice - the [t] option aligns each column's content to the top

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The first column

%----------------------------------------------------------------------------------------
%   Objectives
%----------------------------------------------------------------------------------------

\begin{alertblock}{Objectives}
    Improve time series classification by:
    \begin{itemize}
        \item Using dynamic models to represent time sereis data as points on a Grassmann manifold
        \item Using kernel methods on said manifolds to perform classification while taking
            advantage of the full geometric information
    \end{itemize}
\end{alertblock}

%----------------------------------------------------------------------------------------
%   Introduction
%----------------------------------------------------------------------------------------

\begin{block}{Introduction}
    The objective of this project is to investigated novel solutions to the problem of
    time series classification. This problem is widespread in signal processing and machine
    learning, from identifying the sources of sound or radar signals, to detecting
    algorithmically-generated ``deepfake'' video and audio. \lbr

    We implemented a new method for feature extraction: representing signals as points on a
    Grassmann manifold via dynamic model parametrisation.  This geometric techinque proves to be
    effective at capturing information from high-dimensional signals, performing well in
    classification.
\end{block}

%----------------------------------------------------------------------------------------
%   Dynamic model parametrisation
%----------------------------------------------------------------------------------------

\begin{block}{Dynamic model parametrisation}
    The autoregressive-moving-average (ARMA) is a well-known dynamic model for time series data that
    parametrises a signal $f(t)$ by the equations

    \begin{equation}
        f(t) = Cz(t) + w(t), \quad w(t) \sim \mathcal{N}(0, R)
    \end{equation}
    \begin{equation}
        z(t + 1) = Az(t) + v(t), \quad v(t) \sim \mathcal{N}(0, Q)
    \end{equation}

    where $z \in \mathbb{R}^d$ is the hidden state vector, $d \leq p$ is the hidden state dimension
    \cite{turaga_statistical_2011}. There are widely-used closed form solutions for estimating the
    parameters $A$ and $C$. It can be shown that the expected observation sequence is given by

    \begin{equation}\label{eq:observation}
        \mathbb{E}\left[\begin{pmatrix} f(0) \\ f(1) \\ f(2) \\ \vdots \end{pmatrix}\right]
        = \begin{bmatrix} C \\ CA \\ CA^2 \\ \vdots \end{bmatrix} z(0)
    \end{equation}
\end{block}

%----------------------------------------------------------------------------------------

\end{column} % End of the first column

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % Begin column 2

%----------------------------------------------------------------------------------------
%   Representation on Grassmann manifold
%----------------------------------------------------------------------------------------

\begin{block}{Representation on Grassmann manifold}
    % Traditional machine learning techniques largely assume data that lies in Euclidean space.
    % Increasingly, techniques are being developed that account for non-linear geometric constraints
    % on sampled data. One such technique is to interpret popular dynamic models of time series data
    % as constructing points on the Grassmann manifold (the space of $d$-dimensional subspaces in
    % $\mathbb{R}^n$) \cite{turaga_statistical_2011}.

    From (\ref{eq:observation}), we observe that the expected observations of $f(t)$ lie in the
    column space of the observability matrix $O_\infty = \begin{bmatrix} C^T & (CA)^T & (CA^2)^T &
    \cdots \end{bmatrix}^T$. \lbr

    We approximate the observability matrix by truncating at the $m$th block to form the
    finite observability matrix $O_m \in \mathsf{M}_{mp \times d}$.  Thus, the ARMA model yields a
    representation of a signal as a subspace of Euclidean space, and hence a point on the Grassmann
    manifold. We store each subspace by orthonormalising $O_m$. \cite{turaga_statistical_2011}
    recommends setting $d$ and $m$ to be equal, between 5 and 10.
\end{block}

%----------------------------------------------------------------------------------------
%   Kernel methods on Grassmann manifold
%----------------------------------------------------------------------------------------

\begin{block}{Kernel methods on Grassmann manifold}
    Many machine learning algorithms, such as support vector machines (SVMs) and deep neural
    networks, depend on the Euclidean structure of input data, particularly structures such as norms
    and inner products \cite{jayasumana_kernel_2015}. One way to address this incongruency is to use
    kernel methods that are defined on manifolds. In \cite{jayasumana_kernel_2015} Jayasumana et al.
    generalise the Gaussian radial basis function (RBF) kernel $\exp(-\gamma \norm{x - y}^2)$ to
    manifolds by replacing the Euclidean distance with a positive definite distance function,
    specifically the projection distance on the Grassmann manifold

    \begin{equation}
        d_P([Y_1], [Y_2]) = 2^{-1/2} \norm{Y_1Y_1^T - Y_2Y_2^T}_F
    \end{equation}

    where $[Y_i]$ is the subspace spanned by the columns of $Y_i$, and $Y_1$ and $Y_2$ are
    matrices with orthonormal columns. This yields the projection Gaussian kernel on the Grassmann
    manifold:

    \begin{equation}\label{eq:kernel}
        k_P([Y_1, Y_2]) = \exp(-\gamma d_P^2([Y_1], [Y_2]))
    \end{equation}

    where $\gamma$ is a hyperparameter. This enables the use of algorithms like SVM to classify time
    series data in the form of orthonormal matrices representing points on a Grassmann manifold.
\end{block}

%----------------------------------------------------------------------------------------

\end{column} % End of column 2

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % Begin column 3

%----------------------------------------------------------------------------------------
%   Experiments
%----------------------------------------------------------------------------------------

\begin{block}{Experiments}
    We computed the representation of each signal as a point on the Grassmann manifold
    according to (\ref{eq:observation}), and trained an SVM using the kernel function described in
    (\ref{eq:kernel}) to perform classification.

    We performed these tests on four sets of data, using 60\% of each dataset for training and 40\%
    of testing.
    \begin{itemize}
        \item SUNY EEG Database Data Set, a set of measurements from 64 electrodes placed on
            the scalp of alcoholic and non-alcoholic subjects \cite{zhang_event_1995} \\
            $d = m = 9, \gamma = 0.2$
        \item Epileptologie Bonn EEG data set, a set of single-channel measurements from epileptic
            and non-epileptic patients \cite{andrzejak_indications_2001}
        \item Audio recordings of different vehicles moving through a parking lot at approximately
            15 mph \cite{sunu_dimensionality_2018}
        \item Video recordings of people speaking numeric digits 0-9 \cite{lieu_signal_2011}
    \end{itemize}
\end{block}

%----------------------------------------------------------------------------------------
%   Results
%----------------------------------------------------------------------------------------

\begin{block}{Results}
    \begin{table}
        \begin{tabular}{l r r}
            \toprule
            \textbf{Datasets} & Accuracy & Feature dim. \\
            \midrule
            Alcohol EEG   & 99.8\% & 64 \\
            Epilepsy EEG  & 66\%   & 1 \\
            Vehicle audio & 63\%   & 2 \\
            Video digits  & 98\%   & 3850 \\
            \bottomrule
        \end{tabular}
        \caption{Classification accuracies and feature dimensions for each dataset}
        \label{results}
    \end{table}
\end{block}

%----------------------------------------------------------------------------------------

\end{column} % End of column 3

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % Begin column 4

%----------------------------------------------------------------------------------------
%   Conclusions
%----------------------------------------------------------------------------------------

\begin{block}{Conclusions}
    We conclude that the approach of representing time series data as points on a Grassmann manifold
    via ARMA parameterisation, then applying a manifold kernel SVM for classification is highly
    effective for datasets with a large number of features, namely the alcohol EEG and video digit
    datasets.

    The Grassmann SVM approach performs relatively poorly on datasets with fewer features, such as
    the epilepsy EEG dataset and the vehicle audio dataset, which comprise 1 and 2 features
    respectively. More sophisticated pre-processing techniques, such Fourier or wavelet
    transformations and instantaneous frequency decompositions, can help extract more relevant
    features from each signal and improve the performance of the Grassmann classification algorithm.
\end{block}

%----------------------------------------------------------------------------------------
%   References
%----------------------------------------------------------------------------------------

\begin{block}{References}

% \nocite{*} % Insert publications even if they are not cited in the poster
\footnotesize{\bibliographystyle{unsrt}
\bibliography{citations}\vspace{0.75in}}

\end{block}

%----------------------------------------------------------------------------------------

\end{column}    % End column 4

%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

% \setbeamercolor{block title}{fg=red,bg=white} % Change the block title color
% 
% \begin{block}{Acknowledgements}
% 
% \small{\rmfamily{Nam mollis tristique neque eu luctus. Suspendisse rutrum congue nisi sed convallis. Aenean id neque dolor. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.}} \\
% 
% \end{block}

%----------------------------------------------------------------------------------------
%	CONTACT INFORMATION
%----------------------------------------------------------------------------------------

% \setbeamercolor{block alerted title}{fg=black,bg=norange} % Change the alert block title colors
% \setbeamercolor{block alerted body}{fg=black,bg=white} % Change the alert block body colors
% 
% \begin{alertblock}{Contact Information}
% 
% \begin{itemize}
% \item Web: \href{http://www.university.edu/smithlab}{http://www.university.edu/smithlab}
% \item Email: \href{mailto:john@smith.com}{john@smith.com}
% \item Phone: +1 (000) 111 1111
% \end{itemize}
% 
% \end{alertblock}
% 
% \begin{center}
% \begin{tabular}{ccc}
% \includegraphics[width=0.4\linewidth]{logo.png} & \hfill & \includegraphics[width=0.4\linewidth]{logo.png}
% \end{tabular}
% \end{center}

% \begin{column}{\onecolwid}\vspace{-.6in} % The second column within column 2 (column 2.2)
% 
% \end{column} % End of column 2.2

% \end{columns} % End of the split of column 2 - any content after this will now take up 2 columns width

%----------------------------------------------------------------------------------------
%	IMPORTANT RESULT
%----------------------------------------------------------------------------------------

% \begin{alertblock}{Important Result}
% 
% Lorem ipsum dolor \textbf{sit amet}, consectetur adipiscing elit. Sed commodo molestie porta. Sed ultrices scelerisque sapien ac commodo. Donec ut volutpat elit.
% 
% \end{alertblock} 

%----------------------------------------------------------------------------------------

% \begin{columns}[t,totalwidth=\twocolwid] % Split up the two columns wide column again
% 
% \begin{column}{\onecolwid} % The first column within column 2 (column 2.1)


%----------------------------------------------------------------------------------------

% \end{column} % End of column 2.1
% 
% \begin{column}{\onecolwid} % The second column within column 2 (column 2.2)

%----------------------------------------------------------------------------------------
%	RESULTS
%----------------------------------------------------------------------------------------

% \begin{block}{Results}
% 
% \begin{figure}
% \includegraphics[width=0.8\linewidth]{placeholder.jpg}
% \caption{Figure caption}
% \end{figure}
% 
% Nunc tempus venenatis facilisis. Curabitur suscipit consequat eros non porttitor. Sed a massa dolor, id ornare enim:
% 
% \begin{table}
% \vspace{2ex}
% \begin{tabular}{l l l}
% \toprule
% \textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
% \midrule
% Treatment 1 & 0.0003262 & 0.562 \\
% Treatment 2 & 0.0015681 & 0.910 \\
% Treatment 3 & 0.0009271 & 0.296 \\
% \bottomrule
% \end{tabular}
% \caption{Table caption}
% \end{table}
% 
% \end{block}

%----------------------------------------------------------------------------------------

% \end{column} % End of column 2.2
% 
% \end{columns} % End of the split of column 2
% 
% \end{column} % End of the second column

%----------------------------------------------------------------------------------------

\end{columns} % End of all the columns in the poster

\end{frame} % End of the enclosing frame

\end{document}
